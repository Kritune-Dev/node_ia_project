#!/usr/bin/env node

/**
 * Script d'installation automatique des modÃ¨les LLM mÃ©dicaux
 * Utilise l'API Ollama pour tÃ©lÃ©charger les modÃ¨les nÃ©cessaires
 */

const https = require('https')
const http = require('http')

const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11436'

const MEDICAL_MODELS = [
  'llama3.2:3b',
  'phi3:mini', 
  'qwen2:7b',
  'gemma2:latest',
  'mistral:latest',
  'tinyllama:latest',
  'orca2:latest',
  'meditron:latest',
  'medllama2:latest',
  'cniongolo/biomistral:latest'
]

async function makeRequest(url, method = 'GET', data = null) {
  return new Promise((resolve, reject) => {
    const urlObj = new URL(url)
    const options = {
      hostname: urlObj.hostname,
      port: urlObj.port,
      path: urlObj.pathname + urlObj.search,
      method: method,
      headers: {
        'Content-Type': 'application/json',
      }
    }

    if (data) {
      const jsonData = JSON.stringify(data)
      options.headers['Content-Length'] = Buffer.byteLength(jsonData)
    }

    const client = urlObj.protocol === 'https:' ? https : http
    const req = client.request(options, (res) => {
      let responseData = ''
      
      res.on('data', (chunk) => {
        responseData += chunk
      })
      
      res.on('end', () => {
        try {
          const parsed = responseData ? JSON.parse(responseData) : {}
          resolve({ status: res.statusCode, data: parsed })
        } catch (e) {
          resolve({ status: res.statusCode, data: responseData })
        }
      })
    })

    req.on('error', (error) => {
      reject(error)
    })

    if (data) {
      req.write(JSON.stringify(data))
    }

    req.end()
  })
}

async function checkOllamaHealth() {
  try {
    console.log('ðŸ” VÃ©rification de la connexion Ollama...')
    const response = await makeRequest(`${OLLAMA_BASE_URL}/api/tags`)
    
    if (response.status === 200) {
      console.log('âœ… Ollama est accessible')
      return true
    } else {
      console.log(`âŒ Ollama non accessible (status: ${response.status})`)
      return false
    }
  } catch (error) {
    console.log(`âŒ Erreur de connexion Ollama: ${error.message}`)
    return false
  }
}

async function getInstalledModels() {
  try {
    const response = await makeRequest(`${OLLAMA_BASE_URL}/api/tags`)
    if (response.status === 200 && response.data.models) {
      return response.data.models.map(model => model.name)
    }
    return []
  } catch (error) {
    console.log(`âŒ Erreur lors de la rÃ©cupÃ©ration des modÃ¨les: ${error.message}`)
    return []
  }
}

async function installModel(modelName) {
  try {
    console.log(`ðŸ“¦ Installation de ${modelName}...`)
    
    const response = await makeRequest(`${OLLAMA_BASE_URL}/api/pull`, 'POST', {
      name: modelName,
      stream: false
    })

    if (response.status === 200) {
      console.log(`âœ… ${modelName} installÃ© avec succÃ¨s`)
      return true
    } else {
      console.log(`âŒ Ã‰chec de l'installation de ${modelName} (status: ${response.status})`)
      return false
    }
  } catch (error) {
    console.log(`âŒ Erreur lors de l'installation de ${modelName}: ${error.message}`)
    return false
  }
}

async function main() {
  console.log('ðŸš€ Installation des modÃ¨les LLM mÃ©dicaux')
  console.log('==========================================')
  
  // VÃ©rifier la connexion Ollama
  const isHealthy = await checkOllamaHealth()
  if (!isHealthy) {
    console.log('\nâŒ Impossible de se connecter Ã  Ollama.')
    console.log('Assurez-vous que Docker Compose est dÃ©marrÃ©:')
    console.log('  docker-compose up -d')
    process.exit(1)
  }

  // RÃ©cupÃ©rer les modÃ¨les dÃ©jÃ  installÃ©s
  console.log('\nðŸ” VÃ©rification des modÃ¨les installÃ©s...')
  const installedModels = await getInstalledModels()
  console.log(`ðŸ“Š ${installedModels.length} modÃ¨le(s) dÃ©jÃ  installÃ©(s)`)

  // DÃ©terminer les modÃ¨les Ã  installer
  const modelsToInstall = MEDICAL_MODELS.filter(model => {
    const isInstalled = installedModels.some(installed => 
      installed === model || installed.startsWith(model.split(':')[0])
    )
    return !isInstalled
  })

  if (modelsToInstall.length === 0) {
    console.log('\nâœ… Tous les modÃ¨les sont dÃ©jÃ  installÃ©s!')
    process.exit(0)
  }

  console.log(`\nðŸ“‹ ${modelsToInstall.length} modÃ¨le(s) Ã  installer:`)
  modelsToInstall.forEach(model => console.log(`  - ${model}`))

  // Installation des modÃ¨les
  console.log('\nâš¡ DÃ©but de l\'installation...')
  let successCount = 0
  let failedModels = []

  for (const model of modelsToInstall) {
    const success = await installModel(model)
    if (success) {
      successCount++
    } else {
      failedModels.push(model)
    }
  }

  // RÃ©sumÃ©
  console.log('\nðŸ“Š RÃ‰SUMÃ‰ DE L\'INSTALLATION')
  console.log('============================')
  console.log(`âœ… ModÃ¨les installÃ©s avec succÃ¨s: ${successCount}/${modelsToInstall.length}`)
  
  if (failedModels.length > 0) {
    console.log(`âŒ ModÃ¨les en Ã©chec: ${failedModels.length}`)
    failedModels.forEach(model => console.log(`  - ${model}`))
    console.log('\nðŸ’¡ Conseils pour les Ã©checs:')
    console.log('  - VÃ©rifiez votre connexion internet')
    console.log('  - Certains modÃ¨les peuvent Ãªtre indisponibles')
    console.log('  - Relancez le script plus tard')
  }

  console.log('\nðŸŽ‰ Installation terminÃ©e!')
  console.log('Vous pouvez maintenant utiliser la plateforme d\'analyse IA.')
}

// Gestion des signaux pour une sortie propre
process.on('SIGINT', () => {
  console.log('\n\nâš ï¸  Installation interrompue par l\'utilisateur')
  process.exit(0)
})

process.on('SIGTERM', () => {
  console.log('\n\nâš ï¸  Installation interrompue')
  process.exit(0)
})

// Lancement du script
if (require.main === module) {
  main().catch(error => {
    console.error('\nðŸ’¥ Erreur critique:', error.message)
    process.exit(1)
  })
}

module.exports = { main, checkOllamaHealth, installModel }
