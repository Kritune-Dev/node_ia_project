import { NextResponse } from 'next/server'
import path from 'path'
import fs from 'fs'

/**
 * üìã API BENCHMARK CONFIGS v2.0 - Configuration des types de tests
 * GET /api/benchmark/configs - Liste des configurations depuis benchmark-configs.json
 * POST /api/benchmark/configs - Ajouter une nouvelle configuration
 */

const BENCHMARK_CONFIGS_FILE = path.join(process.cwd(), 'data', 'benchmark-configs.json')

/**
 * üìñ Charger les configurations depuis le fichier JSON
 */
async function loadBenchmarkConfigs(): Promise<any> {
  try {
    const data = fs.readFileSync(BENCHMARK_CONFIGS_FILE, 'utf-8')
    return JSON.parse(data)
  } catch (error) {
    console.error('‚ùå [BENCHMARK-CONFIGS] Erreur lecture fichier:', error)
    return { benchmarks: {}, metadata: {} }
  }
}

/**
 * üíæ Sauvegarder les configurations dans le fichier JSON
 */
async function saveBenchmarkConfigs(configs: any): Promise<void> {
  try {
    fs.writeFileSync(BENCHMARK_CONFIGS_FILE, JSON.stringify(configs, null, 2))
    console.log('‚úÖ [BENCHMARK-CONFIGS] Fichier sauvegard√©')
  } catch (error) {
    console.error('‚ùå [BENCHMARK-CONFIGS] Erreur sauvegarde:', error)
    throw error
  }
}

/**
 * GET - R√©cup√©rer toutes les configurations de benchmark
 */
export async function GET() {
  console.log('üìã [BENCHMARK-CONFIGS] GET - R√©cup√©ration des configurations depuis JSON')

  try {
    const configData = await loadBenchmarkConfigs()
    const benchmarks = configData.benchmarks || {}
    
    // Convertir l'objet en tableau avec les informations essentielles
    const configs = Object.values(benchmarks).map((benchmark: any) => ({
      id: benchmark.id,
      name: benchmark.name,
      description: benchmark.description,
      estimatedTime: benchmark.parameters?.timeout ? Math.round(benchmark.parameters.timeout / 1000) : 60,
      questionCount: benchmark.questions?.length || 0,
      difficulty: benchmark.questions?.length <= 3 ? 'easy' : 
                  benchmark.questions?.length <= 5 ? 'medium' : 'hard',
      category: benchmark.testTypes?.[0] || 'general',
      version: benchmark.version || '1.0.0',
      testTypes: benchmark.testTypes || [],
      parameters: benchmark.parameters || {},
      questions: benchmark.questions || [],
      scoring: benchmark.scoring || {}
    }))

    console.log(`‚úÖ [BENCHMARK-CONFIGS] ${configs.length} configurations retourn√©es depuis JSON`)

    return NextResponse.json({
      success: true,
      configs,
      count: configs.length,
      metadata: configData.metadata || {},
      timestamp: new Date().toISOString()
    })

  } catch (error) {
    console.error('‚ùå [BENCHMARK-CONFIGS] Erreur:', error)
    
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'Erreur inconnue'
    }, { status: 500 })
  }
}

/**
 * POST - Ajouter une nouvelle configuration de benchmark
 */
export async function POST(request: Request) {
  console.log('üìã [BENCHMARK-CONFIGS] POST - Ajout d\'une nouvelle configuration')

  try {
    const newBenchmark = await request.json()
    
    // Validation des champs requis
    if (!newBenchmark.id || !newBenchmark.name || !newBenchmark.description) {
      return NextResponse.json({
        success: false,
        error: 'Champs requis manquants: id, name, description'
      }, { status: 400 })
    }

    // Charger les configurations existantes
    const configData = await loadBenchmarkConfigs()
    
    // V√©rifier si l'ID existe d√©j√†
    if (configData.benchmarks && configData.benchmarks[newBenchmark.id]) {
      return NextResponse.json({
        success: false,
        error: `Un benchmark avec l'ID '${newBenchmark.id}' existe d√©j√†`
      }, { status: 409 })
    }

    // Pr√©parer la nouvelle configuration avec des valeurs par d√©faut
    const benchmarkConfig = {
      id: newBenchmark.id,
      name: newBenchmark.name,
      description: newBenchmark.description,
      version: newBenchmark.version || "1.0.0",
      testTypes: newBenchmark.testTypes || ["qualitative"],
      parameters: {
        temperature: newBenchmark.parameters?.temperature || 0.3,
        seed: newBenchmark.parameters?.seed || Math.floor(Math.random() * 1000),
        timeout: newBenchmark.parameters?.timeout || 60000,
        maxTokens: newBenchmark.parameters?.maxTokens || 500,
        topP: newBenchmark.parameters?.topP || 0.9,
        ...newBenchmark.parameters
      },
      prompts: {
        system: newBenchmark.prompts?.system || "Tu es un assistant IA expert. R√©ponds avec pr√©cision et clart√©.",
        evaluation: newBenchmark.prompts?.evaluation || "√âvalue cette r√©ponse sur une √©chelle de 1 √† 10.",
        ...newBenchmark.prompts
      },
      questions: newBenchmark.questions || [],
      scoring: {
        excellent: 10,
        good: 8,
        average: 6,
        poor: 4,
        incorrect: 2,
        noResponse: 0,
        ...newBenchmark.scoring
      }
    }

    // Ajouter la nouvelle configuration
    if (!configData.benchmarks) {
      configData.benchmarks = {}
    }
    configData.benchmarks[newBenchmark.id] = benchmarkConfig

    // Mettre √† jour les m√©tadonn√©es
    if (!configData.metadata) {
      configData.metadata = {}
    }
    configData.metadata.lastUpdated = new Date().toISOString()
    configData.metadata.totalBenchmarks = Object.keys(configData.benchmarks).length

    // Sauvegarder dans le fichier
    await saveBenchmarkConfigs(configData)

    console.log(`‚úÖ [BENCHMARK-CONFIGS] Nouveau benchmark '${newBenchmark.id}' ajout√©`)

    return NextResponse.json({
      success: true,
      message: `Benchmark '${newBenchmark.id}' ajout√© avec succ√®s`,
      benchmark: benchmarkConfig,
      timestamp: new Date().toISOString()
    }, { status: 201 })

  } catch (error) {
    console.error('‚ùå [BENCHMARK-CONFIGS] Erreur POST:', error)
    
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'Erreur inconnue'
    }, { status: 500 })
  }
}
